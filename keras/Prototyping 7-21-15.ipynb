{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox for playing around with models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "import os.path as path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr\n",
    "# Keras imports\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l1, l2, activity_l1, activity_l2\n",
    "from keras.callbacks import Callback\n",
    "#Imports to add Poisson objective (since Keras does not have them)\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from six.moves import range\n",
    "import socket\n",
    "import getpass\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(x=np.linspace(-5,5,50),sigma=1.,mu=0.):\n",
    "     return np.array([(1./(2.*np.pi*sigma**2))*np.exp((-(xi-mu)**2.)/(2.*sigma**2)) for xi in x])\n",
    "\n",
    "def rolling_window(array, window):\n",
    "    \"\"\"\n",
    "    Make an ndarray with a rolling window of the last dimension\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array to add rolling window to\n",
    "    window : int\n",
    "        Size of rolling window\n",
    "    Returns\n",
    "    -------\n",
    "    Array that is a view of the original array with a added dimension\n",
    "    of size w.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> x=np.arange(10).reshape((2,5))\n",
    "    >>> rolling_window(x, 3)\n",
    "    array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],\n",
    "           [[5, 6, 7], [6, 7, 8], [7, 8, 9]]])\n",
    "    Calculate rolling mean of last dimension:\n",
    "    >>> np.mean(rolling_window(x, 3), -1)\n",
    "    array([[ 1.,  2.,  3.],\n",
    "           [ 6.,  7.,  8.]])\n",
    "    \"\"\"\n",
    "    assert window >= 1, \"`window` must be at least 1.\"\n",
    "    assert window < array.shape[-1], \"`window` is too long.\"\n",
    "\n",
    "    # # with strides\n",
    "    shape = array.shape[:-1] + (array.shape[-1] - window, window)\n",
    "    strides = array.strides + (array.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(array, shape=shape, strides=strides)\n",
    "\n",
    "def loadData(data_dir):\n",
    "    metadata = np.load(path.join(data_dir, 'metadata.npz'))['metadata'].item()\n",
    "    stim  = np.memmap(path.join(data_dir, 'stim_norm.dat'), dtype=metadata['stim_norm_dtype'], \n",
    "                      mode='r', shape=metadata['stim_norm_shape'])\n",
    "    rates = np.memmap(path.join(data_dir, 'rates.dat'), dtype=metadata['rates_dtype'], \n",
    "                      mode='r', shape=metadata['rates_shape'])\n",
    "    #Smooth raw spike count with 10 ms std Gaussian to get PSTHs\n",
    "    rates_filt = np.zeros(rates.shape)\n",
    "    filt = gaussian(x=np.linspace(-5,5,10), sigma=1, mu=0)\n",
    "    for cell in xrange(rates.shape[1]):\n",
    "        rates_filt[:,cell] = np.convolve(rates[:,cell], filt, mode='same')\n",
    "    #Create 4d stim array where each data point is a 400ms (40 frame) movie\n",
    "    stim_sliced = stim[34:-34, 34:-34,:]\n",
    "    X = rolling_window(stim_sliced, 40)\n",
    "    X = np.rollaxis(X, 2)\n",
    "    X = np.rollaxis(X, 3, 1)\n",
    "    #Truncate rates to appropriate time frame\n",
    "    y = rates_filt[X.shape[1]:]\n",
    "    return X, y\n",
    "\n",
    "def createTrainValTest(X, y, cell):\n",
    "    # Divide examples into training, validation, and test sets\n",
    "    # don't need to zero mean data since we loaded stim_norm\n",
    "    numTrain = 50000\n",
    "    numVal   = 500\n",
    "    numTest  = 500\n",
    "\n",
    "    drawIndices = np.random.choice(X.shape[0], size=(numTrain+numVal+numTest), replace=False)\n",
    "    trainMask = drawIndices[:numTrain]\n",
    "    # valMask   = drawIndices[numTrain:-numTest]\n",
    "    testMask = drawIndices[-numTest:]\n",
    "    X_train = X[trainMask,:,:,:] #will use validation split to hold out random 500 examples for valset\n",
    "    y_train = y[trainMask,cell]\n",
    "    X_test = X[testMask,:,:,:]\n",
    "    y_test = y[testMask,cell]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def poisson_loss(y_true, y_pred):\n",
    "    #Negative log likelihood of data y_true given predictions y_pred, according to a Poisson model\n",
    "    #Assumes that y_pred is > 0\n",
    "\n",
    "    return T.mean(y_pred - y_true * T.log(y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_basename = 'three_layer_convnet_weights'\n",
    "num_epochs = 1\n",
    "learning_rate=5e-5\n",
    "decay_rate=0.99\n",
    "batch_size=50\n",
    "val_split=0.01\n",
    "filter_size=9\n",
    "num_filters=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data and test data...\n",
      "(This might take awhile)\n",
      "(50000, 40, 32, 32)\n",
      "(50000,)\n",
      "(500, 40, 32, 32)\n",
      "(500,)\n",
      "Training and test data loaded. Onto training for 1 epochs...\n"
     ]
    }
   ],
   "source": [
    "print \"Loading training data and test data...\"\n",
    "print \"(This might take awhile)\"\n",
    "if socket.gethostname() == 'lane.local':\n",
    "    data_dir = path.expanduser('~/Git/deepRGC/datasets/white_noise/')\n",
    "elif socket.gethostname() in ['rye01.stanford.edu', 'rye02.stanford.edu']:\n",
    "    username = getpass.getuser()\n",
    "    data_dir = '/farmshare/user_data/%s/white_noise/' %(username)\n",
    "[X, y] = loadData(data_dir)\n",
    "cell = 9\n",
    "[X_train, y_train, X_test, y_test] = createTrainValTest(X, y, cell)\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape\n",
    "print \"Training and test data loaded. Onto training for \" + str(num_epochs) + \" epochs...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Constants ###########\n",
    "num_channels = 40\n",
    "\n",
    "########### Initialize Feedforward Convnet ###########\n",
    "model = Sequential()\n",
    "\n",
    "########### Layer 1 ###########\n",
    "# conv-relu-pool layer\n",
    "#border_mode = full is the default scipy.signal.convolve2d value to do a full linear convolution of input\n",
    "#subsample=(1,1) gives a stride of 1\n",
    "model.add(Convolution2D(num_filters, num_channels, filter_size, filter_size, \n",
    "    init='normal', border_mode='full', subsample=(1,1), W_regularizer=l2(0.0))) \n",
    "model.add(Activation('relu'))\n",
    "#ignore_border is the default, since usually not ignoring the border results in weirdness\n",
    "model.add(MaxPooling2D(poolsize=(2, 2), ignore_border=True))\n",
    "# model.add(Dropout(0.25)) #example of adding dropout\n",
    "\n",
    "########### Layer 2 ###########    \n",
    "# affine-relu layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6400, 32, init='normal', W_regularizer=l2(0.0)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "########### Layer 3 ###########    \n",
    "# affine-softplus layer\n",
    "model.add(Dense(32, 1, init='normal', W_regularizer=l2(0.0)))\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "\n",
    "########### Loss Function ###########    \n",
    "#Default values (recommended) of RMSprop are learning rate=0.001, rho=0.9, epsilon=1e-6\n",
    "#holds out 500 of the 50000 training examples for validation\n",
    "# rho is decay rate, not sure what epsilon is, so keeping that at default.\n",
    "# other hyperparameters taken from python script\n",
    "rmsprop = RMSprop(lr=learning_rate, rho=decay_rate, epsilon=1e-6)\n",
    "model.compile(loss=poisson_loss, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49500 samples, validate on 500 samples\n",
      "Epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "error copying data to device memory\nApply node that caused the error: GpuFromHost(<TensorType(float32, 4D)>)\nInputs types: [TensorType(float32, 4D)]\nInputs shapes: [(50, 40, 32, 32)]\nInputs strides: [(163840, 4, 5120, 160)]\nInputs values: ['not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5a9d9b191cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLossHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=num_epochs, \n\u001b[0;32m----> 5\u001b[0;31m         verbose=1, validation_split=val_split, callbacks=[history])\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/Keras-0.1.2-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=callbacks, \\\n\u001b[0;32m--> 413\u001b[0;31m             validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/Keras-0.1.2-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, validation_split, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0;31m# For the c linker We don't have access from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/EPD64.framework/Versions/7.3/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: error copying data to device memory\nApply node that caused the error: GpuFromHost(<TensorType(float32, 4D)>)\nInputs types: [TensorType(float32, 4D)]\nInputs shapes: [(50, 40, 32, 32)]\nInputs strides: [(163840, 4, 5120, 160)]\nInputs values: ['not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "########### Fit Model with Callbacks ###########    \n",
    "# initialize empty list of loss history\n",
    "history = LossHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=num_epochs, \n",
    "        verbose=1, validation_split=val_split, callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Post-training Evaluation and Visualization ###########    \n",
    "#saves the weights to HDF5 for potential later use\n",
    "model.save_weights(model_basename + str(num_epochs), overwrite=True)\n",
    "#Would not need accuracy since that is for classification (e.g. F1 score), whereas our problem is regression,\n",
    "#so likely we will set show_accuracy=False\n",
    "score = model.evaluate(X_test, y_test, show_accuracy=False, verbose=1)\n",
    "print('Test score:', score)\n",
    "#save test score\n",
    "pickle.dump(score, open(model_basename + str(num_epochs) + \"_testsetscore.p\", \"wb\"))\n",
    "\n",
    "# Figure to visualize loss history after each batch\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((20,24))\n",
    "ax = plt.subplot()\n",
    "ax.plot(history.losses, 'k')\n",
    "ax.set_title('Loss history', fontsize=16)\n",
    "ax.set_xlabel('Iteration', fontsize=16)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = '%dEpochs.png' %(num_epochs)\n",
    "plt.savefig(filename, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
